# 第一部分 介绍深度学习
深度学习的三部分：神经网络架构、学习目标、学习

在第三步学习中，需要寻找到最佳的功能函数。

从函数的角度去理解：第一步、构造一个函数集；第二步、定义函数的拟合度；第三步、选择最佳函数
# 关于训练深度神经网络的一些建议
损失函数

小批度梯度下降

激活函数

调整学习率


## 为防止过拟合
可以提前终止训练步骤

正则化：权重衰减，在权重之前乘以某一系数

删除节点：每层神经元在更新之前有p%的概率被去除掉；还可以将其理解成一种集成方法。（因为在使用小批度梯度下降时，因选择的节点不同，去除的神经元也就不一样）

合适的神经网络结构如（cnn, 属于第一步，选择合适的神经网络）
# 各种各样的神经网络
## 卷积神经网络（CNN)
为何cnn适用于图片:

图片的输入层节点庞大，在判断时（如判断鸟嘴），多个神经元都做了重复的工作（因为不确定鸟嘴在哪里），在cnn中只用一个神经元来判断。而且像素的缩小不影响cnn的处理，可以节省空间。

卷积的目标：1，特征维度小于整个图片。2，特征在多个图片中都出现。

最大池化的目标：将像素降低不影响目标。

卷积：将原始的数据进行通过滑窗进行分割，与设置好的特征进行匹配，返回匹配的程度，当一个样本与所有特征都匹配结束之后，就得到了一个数据对应的特征集合。

零补充：通过零补充的方式可以得到与分隔之前大小相同的数据。

最大池：在滑窗返回的特征中，按照规律将区域分隔，在某一区域中选取匹配程度最高的作为当前区域的匹配程度。

经过卷积和最大池之后得到了一个新的特征，通过不断重复上述步骤，将最后结果作为特征，拉成向量后，作为输入，加到神经网络中，即为cnn.

## 循环神经网络（RNN)
将隐藏层进行储存，可以作为输入层进行输入。

# 下一股浪潮
极深神经网络：是具有不同深度的神经网络的集成，

注意力模型：

加强学习
# 神经网络与深度学习（吴恩达课程）
## 第一章
rnn通常被应用于序列化的数据中。

机器学习采用的数据是结构化数据，而深度学习可以使用非结构化数据。

当数据集较小时，深度学习的优越性难以体现，唯有数据量极为庞大时，深度学习的优越性才会展现出来。

数据和计算能力的提升促进了深度学习的大发展，而近些年来算法的提升渐渐体现，而算法的提升也体现在提升了计算速度上。如（将sigmod函数替换成ReLU函数，这样会加快梯度下降法的速度）

深度学习的输入：在图片作为有色输入时，分为红、绿、蓝三张像素值，即每一帧图片都对应着三张图片。
## 第二章
反向传播是为了计算损失函数的导数。损失函数对y-hat的结果容易求得，对sigmod函数的求导为，z(1-z),损失函数对z的求导为 z-y （由链式法则）。

通过向量化，替代for循环从而加快运行速度。如通过np.dot(w,x)代替了一系列w和x的相乘；np.exp np.log np.abs np.maximum 全部可以代替for循环，提升速度。

广播：在numpy中，当一个矩阵(m,n)与另一个矩阵(m,1) (1,n) 进行加减乘除时，会自动地将低维矩阵自身复制拓展到和高维矩阵相等，再进行计算。

通过reshape确保运算时矩阵的维数正确，a=a.reshape(1,5),通过assert(a.shape==(5,1)) 检查矩阵形状是否正确

## 第三章
神经网络第一层输入是特征，每一层的神经元都是一个logstic单元，多层即是重复计算。也有使用ReLU单元作为基本单元的。隐藏层的训练单元个数也是由学习过程中取舍的。将输入向量进行列堆叠，可以实现向量化计算。

激活函数的选择：tanh函数，即双曲正切函数。

![image](https://github.com/lichenxiuxing/deep-learning/blob/master/TIM截图20190614133809.png)

